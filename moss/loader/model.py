#!/usr/bin/env python
#-*- coding:utf-8 -*-


import torch
import warnings

from transformers import LlamaForCausalLM
from transformers import AutoModelForCausalLM
from transformers import AutoModel, AutoTokenizer


def raise_warning_for_old_weights(model_path, model):
    if "vicuna" in model_path.lower():
        try:
            is_vicuna = isinstance(model, LlamaForCausalLM)
        except Exception:
            is_vicuna = isinstance(model, LLamaForCausalLM)
        if is_vicuna and model.model.vocab_size > 32000:
            warnings.warn(
                "\nYou are probably using the old Vicuna-v0 model, "
                "which will generate unexpected results with the "
                "current fschat.\nYou can try one of the following methods:\n"
                "1. Upgrade your weights to the new Vicuna-v1.1: https://github.com/lm-sys/FastChat#vicuna-weights.\n"
                "2. Use the old conversation template by `python3 -m fastchat.serve.cli --model-path /path/to/vicuna-v0 --conv-template conv_one_shot`\n"
                "3. Downgrade fschat to fschat==0.1.10 (Not recommonded).\n")


def load_model(model_path, device, num_gpus, max_gpu_memory="13GiB",
               load_8bit=False, debug=False):
    if device == "cpu":
        kwargs = {}
    elif device == "cuda":
        kwargs = {"torch_dtype": torch.float16}
        if num_gpus == "auto":
            kwargs["device_map"] = "auto"
        else:
            num_gpus = int(num_gpus)
            if num_gpus != 1:
                kwargs.update({
                    "device_map": "auto",
                    "max_memory": {i: max_gpu_memory for i in range(num_gpus)},
                })
    # elif device == "mps":
    #     kwargs = {"torch_dtype": torch.float16}
    #     # Avoid bugs in mps backend by not using in-place operations.
    #     replace_llama_attn_with_non_inplace_operations()
    else:
        raise ValueError(f"Invalid device: {device}")

    if "chatglm" in model_path:
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()
    elif "dolly" in model_path:
        kwargs.update({"torch_dtype": torch.bfloat16})
        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
        # 50277 means "### End"
        tokenizer.eos_token_id = 50277
        model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)
    else:
        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
        model = AutoModelForCausalLM.from_pretrained(model_path,
            low_cpu_mem_usage=True, **kwargs)
        raise_warning_for_old_weights(model_path, model)

    # if load_8bit:
    #     compress_module(model, device)

    if (device == "cuda" and num_gpus == 1) or device == "mps":
        model.to(device)

    if debug:
        print(model)

    return model, tokenizer