| Dataset | Description | Size | Link |
| ------- | ----------- | ---- | ---- |
| Project Gutenberg | A digital library of over 60,000 free e-books that are in the public domain, including classic works of literature and more obscure works. | Over 60,000 books | https://www.gutenberg.org/ |
| Smashwords | A self-publishing platform for independent authors that allows them to distribute their works in various e-book formats, including fiction, non-fiction, romance, and science fiction. | Over 500,000 books | https://www.smashwords.com/ |
| BookCorpus/BookCorpus (Toronto) | A dataset of over 11,000 books created by scraping the text of free e-books from the web, intended for use in natural language processing and machine learning research. | Over 11,000 books | https://yknzhu.wixsite.com/mbweb |
| Wikipedia Dump | A compressed XML file that contains the full text of all Wikipedia articles, including the revision history and discussion pages. | Several terabytes | https://dumps.wikimedia.org/ |
| WebText | A large collection of web pages scraped from URLs shared on Reddit, covering a diverse set of topics and genres. | Over 8 million documents | https://huggingface.co/datasets/webtext |
| Common Crawl | A dataset of web pages collected by crawling the internet, which includes a diverse set of sources and languages. | Petabytes of data | http://commoncrawl.org/ |
| SPGC | A large-scale corpus of Chinese text collected from a wide range of sources, including news articles, social media, and online forums. | Over 3 billion words | https://github.com/isnowfy/sogou-dl-dataset |
| Common Crawl | A dataset of web pages collected by crawling the internet, which includes a diverse set of sources and languages. | Petabytes of data | http://commoncrawl.org/ |